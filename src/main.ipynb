{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2850a35-0e8c-4e6b-a44d-36921346bee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:31.112623Z",
     "iopub.status.busy": "2023-08-18T10:11:31.112623Z",
     "iopub.status.idle": "2023-08-18T10:11:37.191160Z",
     "shell.execute_reply": "2023-08-18T10:11:37.190037Z",
     "shell.execute_reply.started": "2023-08-18T10:11:31.112623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import torch\n",
    "\n",
    "import flax\n",
    "from flax import linen  # keras but for flax essentially. More literally, torch.nn but for flax\n",
    "import jax\n",
    "import jaxlib\n",
    "import optax\n",
    "\n",
    "import sklearn\n",
    "\n",
    "# RL\n",
    "import gymnasium as gym  # for environment interaction: states/observations and actions, as well as their associated spaces and the interaction of actions \n",
    "import stable_baselines3\n",
    "import skrl\n",
    "#import tf_agents\n",
    "\n",
    "# Statistics\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2a2a0f-34d7-4ed9-934d-b634b4da6ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.193003Z",
     "iopub.status.busy": "2023-08-18T10:11:37.191662Z",
     "iopub.status.idle": "2023-08-18T10:11:37.206012Z",
     "shell.execute_reply": "2023-08-18T10:11:37.206012Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.191662Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.25.2\n",
      "pandas 1.5.3\n",
      "tensorflow 2.10.0\n",
      "torch 2.0.1\n",
      "flax 0.7.2\n",
      "JAX 0.4.14\n",
      "optax 0.1.7\n",
      "sklearn 1.3.0\n",
      "gymnasium 0.28.1\n",
      "stable baselines3 2.0.0\n",
      "skrl 1.0.0rc2\n",
      "seaborn 0.12.2\n"
     ]
    }
   ],
   "source": [
    "# Check versions (in case of conflicts)\n",
    "# Not always required, but in this case since I imported a million different things with dependencies yeah it's good for debugging\n",
    "print(f\"numpy {np.__version__}\")\n",
    "print(f\"pandas {pd.__version__}\")\n",
    "print(f\"tensorflow {tf.__version__}\")\n",
    "print(f\"torch {torch.__version__}\")\n",
    "print(f\"flax {flax.__version__}\")\n",
    "print(f\"JAX {jax.__version__}\")\n",
    "print(f\"optax {optax.__version__}\")\n",
    "print(f\"sklearn {sklearn.__version__}\")\n",
    "print(f\"gymnasium {gym.__version__}\")\n",
    "print(f\"stable baselines3 {stable_baselines3.__version__}\")\n",
    "print(f\"skrl {skrl.__version__}\")\n",
    "print(f\"seaborn {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a29faf8-f62b-4a72-96aa-4ddcb488e575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.207226Z",
     "iopub.status.busy": "2023-08-18T10:11:37.207226Z",
     "iopub.status.idle": "2023-08-18T10:11:37.236746Z",
     "shell.execute_reply": "2023-08-18T10:11:37.236746Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.207226Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's play with JAX (mostly just numpy but jaxified)\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jnp.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb9f97-3567-411f-b6d4-0171e4850eaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Taxi Example\n",
    "\n",
    "### Goal\n",
    "The taxi will start at a random square. Given the location of a passenger and their destination, its goal is to get to move itself to pick up a passenger (each movement = separate action), and then move to its desired destination to drop off the passenger. Picking up and dropping off are also separate actions. It wants to achieve the goal, and as fast as possible (it's a Taxi for crying out loud!)\n",
    "\n",
    "### Reward Function\n",
    "The reward function:\n",
    "- (-1) per step if no reward; encourages finding the fastest route\n",
    "- (+20) for delivering the passenger correctly\n",
    "- (-10) for doing things incorrectly. That is, executing \"pick up\" and \"drop off\" actions illegally \n",
    "\n",
    "### Episodic Task\n",
    "Ends when terminated or truncated\n",
    "- Terminated if: task successfully completed (dropped off the passenger correctly)\n",
    "- Truncated if: using the `time_limit` wrapper and length of the episode is 200 \n",
    "\n",
    "Link: https://gymnasium.farama.org/environments/toy_text/taxi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6593e0e-34ec-44b0-93e0-0cd3ceedc45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.239962Z",
     "iopub.status.busy": "2023-08-18T10:11:37.238943Z",
     "iopub.status.idle": "2023-08-18T10:11:37.252333Z",
     "shell.execute_reply": "2023-08-18T10:11:37.252333Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.239962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# State Space Dicts\n",
    "# (or Observation Space, but in this case since we know all that is going on in the environment, we can use the term state)\n",
    "PASSENGER_LOCATIONS = [\"Red\", \"Green\", \"Yellow\", \"Blue\", \"In Taxi\"]\n",
    "PASSENGER_DESTINATIONS = [\"Red\", \"Green\", \"Yellow\", \"Blue\"]\n",
    "POSSIBLE_TAXI_POSITIONS = set(range(25))  # unnecessary line, but added for clarification that there are 25 possible taxi positions\n",
    "\n",
    "# Action Space Dict\n",
    "ACTIONS = [\"DOWN\", \"UP\", \"RIGHT\", \"LEFT\", \"PICK UP\", \"DROP OFF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7b67e8-b2be-4fdd-a5aa-9db98f183efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.254574Z",
     "iopub.status.busy": "2023-08-18T10:11:37.253557Z",
     "iopub.status.idle": "2023-08-18T10:11:37.283658Z",
     "shell.execute_reply": "2023-08-18T10:11:37.283658Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.254574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m[skrl:WARNING] Isaac Orbit runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[33;20m[skrl:WARNING] IsaacGymEnvs runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[33;20m[skrl:WARNING] OmniIsaacGymEnvs runs on GPU, but there is no GPU backend for JAX. JAX operations will run on CPU.\u001b[0m\n",
      "\u001b[38;20m[skrl:INFO] Environment class: gymnasium.core.Wrapper, gymnasium.utils.record_constructor.RecordConstructorArgs\u001b[0m\n",
      "\u001b[38;20m[skrl:INFO] Environment wrapper: gymnasium\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_jax': False,\n",
       " '_env': <TimeLimit<OrderEnforcing<PassiveEnvChecker<TaxiEnv<Taxi-v3>>>>>,\n",
       " 'device': CpuDevice(id=0),\n",
       " '_vectorized': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are impatient and want things fast, so let's use JAX (not recommended for beginners, but the code is mostly the same)\n",
    "from skrl.envs.wrappers.jax import wrap_env as wrap_env_jax\n",
    "\n",
    "\n",
    "# Taxi Environment\n",
    "gymnasium_env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env = wrap_env_jax(gymnasium_env, wrapper=\"gymnasium\")  # JAXification!\n",
    "\n",
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d704b49-50f2-4666-a28c-4e9ce748fb95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.285664Z",
     "iopub.status.busy": "2023-08-18T10:11:37.285664Z",
     "iopub.status.idle": "2023-08-18T10:11:37.298679Z",
     "shell.execute_reply": "2023-08-18T10:11:37.298679Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.285664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_env',\n",
       " '_jax',\n",
       " '_observation_to_tensor',\n",
       " '_tensor_to_action',\n",
       " '_vectorized',\n",
       " 'action_space',\n",
       " 'close',\n",
       " 'device',\n",
       " 'num_agents',\n",
       " 'num_envs',\n",
       " 'observation_space',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'state_space',\n",
       " 'step']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It might look like the wrapped env doesn't have the attributes and methods the non-wrapped env has, but it does have access to it!\n",
    "# The point of the wrapped environment is to work with the skrl framework, which provides interopability between multiple environmental frameworks such as Gymnasium, DeepMind, NVIDIA, and others\n",
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecca7c6-dcc9-41c3-a066-67a57dc8270f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.300807Z",
     "iopub.status.busy": "2023-08-18T10:11:37.299805Z",
     "iopub.status.idle": "2023-08-18T10:11:37.314634Z",
     "shell.execute_reply": "2023-08-18T10:11:37.314634Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.300807Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da2e471-4455-46e5-87c3-8d69b338d742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:11:37.397320Z",
     "iopub.status.busy": "2023-08-18T10:11:37.397320Z",
     "iopub.status.idle": "2023-08-18T10:11:37.406821Z",
     "shell.execute_reply": "2023-08-18T10:11:37.406821Z",
     "shell.execute_reply.started": "2023-08-18T10:11:37.397320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f20449f-1eeb-4491-a29f-d6c040732ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:25:14.841949Z",
     "iopub.status.busy": "2023-08-18T10:25:14.841949Z",
     "iopub.status.idle": "2023-08-18T10:25:14.852490Z",
     "shell.execute_reply": "2023-08-18T10:25:14.852490Z",
     "shell.execute_reply.started": "2023-08-18T10:25:14.841949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from skrl.models.jax.base import Model as Model_jax\n",
    "from skrl.models.jax.categorical import CategoricalMixin as CategoricalMixin_jax\n",
    "from skrl.models.jax.deterministic import DeterministicMixin as DeterministicMixin_jax\n",
    "\n",
    "\n",
    "# Let's get the policy prototype(s) straight (creating a skeleton for the policy; this is essentially its mechanical intellectual capacity)\n",
    "# Policy is the function mapping situations to actions\n",
    "\n",
    "# The prefix 'Categorical' refers to the OUTPUT of the policy\n",
    "# The output space is discrete, since the action space is also discrete!\n",
    "# We will be using one that outputs using a Categorical Distribution (or Discrete Probability Distribution). This should make exploration more robust, but the other reason is that skrl is only supporting this when using JAX with discrete spaces\n",
    "# This'll be quite the wild ride so strap on in!\n",
    "# This class will essentially be a prototype for the categorical models/policies used by the agent \n",
    "class CategoricalPolicyModel(CategoricalMixin_jax, Model_jax):\n",
    "    def __init__(self, observation_space, action_space, device=None, unnormalized_log_prob=True, **kwargs):\n",
    "        Model_jax.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        CategoricalMixin_jax.__init__(self, unnormalized_log_prob)\n",
    "    \n",
    "    @nn.compact  # marks the given module method allowing inlined submodules; basically this means I can define nn.Dense(), etc. within this function\n",
    "    def __call__(self, inputs, role):\n",
    "        #print(inputs)\n",
    "        # Maps state/observation to action\n",
    "        x = nn.elu(nn.Dense(32)(inputs[\"states\"]))\n",
    "        x = nn.elu(nn.Dense(16)(x))\n",
    "        x = nn.Dense(self.num_actions)(x)  # 6 neurons in this case\n",
    "        \n",
    "        return x, {}\n",
    "\n",
    "# Deterministic and Continuous OUTPUT which is for the expected long-term value estimation\n",
    "# This model will be used for expected long-term value estimation from the Critic to guide the Actor's exploration'\n",
    "# Actor's exploration will be guided by the Critic's value, creating a hybrid of policy-based and value-based methods\n",
    "# Exploration from the Actor will start out as it just doing random things and seeing what sticks/leads it to the most rewards.\n",
    "# But as exploration goes on, the Critic's long-term value estimation will become increasingly accurate\n",
    "# And now the Actor's exploration won't be as random anymore; it'll start to explore in ways the Critic values highly\n",
    "# This creates a more focused/targeted/productive way to explore that doesn't boil down to pure stochasticity, and helps the Actor learn its policy by learning the long-term importances of its actions in different situations, thereby helping to deal with the Credit Assignment Problem.\n",
    "# This differs from curriculum learning because its a characteristic to the model, not the environment/game it plays. This allows for more robustness. Although, combining both techniques isn't a bad idea by any means.\n",
    "class DeterministicValueModel(DeterministicMixin_jax, Model_jax):\n",
    "    def __init__(self, observation_space, action_space, device=None, clip_actions=False, **kwargs):\n",
    "        Model_jax.__init__(self, observation_space, action_space, device, **kwargs)\n",
    "        DeterministicMixin_jax.__init__(self, clip_actions)\n",
    "        \n",
    "    @nn.compact # marks the given module method allowing inlined submodules; basically this means I can define nn.Dense(), etc. within this function\n",
    "    def __call__(self, inputs, role):\n",
    "        # MLP: Nonlinear Regression\n",
    "        # This Critic will evaluate the actions taken based on the state/observation\n",
    "        \n",
    "        print(inputs)\n",
    "        \n",
    "        # Flatten them; since the states and actions are 1D anyway this is OK\n",
    "        states_flattened = jnp.ravel(inputs[\"states\"])\n",
    "        #taken_actions_flattened = jnp.ravel(inputs[\"taken_actions\"])\n",
    "        \n",
    "        x = states_flattened\n",
    "        #x = jnp.concatenate([states_flattened, taken_actions_flattened], axis=-1)\n",
    "        \n",
    "        #try:\n",
    "        #    taken_actions_flattened = jnp.ravel(inputs[\"taken_actions\"])\n",
    "        \n",
    "        #    x = jnp.concatenate([states_flattened, taken_actions_flattened], axis=-1)\n",
    "        #except:\n",
    "        #    x = states_flattened\n",
    "        \n",
    "        \n",
    "        x = jnp.reshape(x, (-1, 1))\n",
    "        \n",
    "        hl1 = nn.relu(nn.Dense(64)(x))\n",
    "        hl2 = nn.relu(nn.Dense(32)(hl1))\n",
    "        \n",
    "        out = nn.Dense(1)(hl2)\n",
    "        \n",
    "        return out, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a02bd0-aa6d-4fa1-a370-c0675b994545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:25:16.451792Z",
     "iopub.status.busy": "2023-08-18T10:25:16.450789Z",
     "iopub.status.idle": "2023-08-18T10:25:16.583791Z",
     "shell.execute_reply": "2023-08-18T10:25:16.583791Z",
     "shell.execute_reply.started": "2023-08-18T10:25:16.451792Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': array([[90]], dtype=int64), 'taken_actions': 2}\n"
     ]
    }
   ],
   "source": [
    "from skrl.agents.jax.ppo import PPO as PPO_jax\n",
    "from skrl.agents.jax.ppo import PPO_DEFAULT_CONFIG as PPO_DEFAULT_CONFIG_JAX\n",
    "from skrl.memories.jax import RandomMemory as RandomMemory_jax\n",
    "\n",
    "# Choosing an agent/model/policy/action architecture\n",
    "# Let's be a basic bitch and use PPO\n",
    "\n",
    "# models that the agent will use\n",
    "ppo_models = {}\n",
    "ppo_models[\"policy\"] = CategoricalPolicyModel(env.observation_space, env.action_space)  # Actor: Policy\n",
    "ppo_models[\"value\"] = DeterministicValueModel(env.observation_space, env.action_space)  # Critic: Expected Cumulative Value Estimation; only required during training\n",
    "\n",
    "# instantiate models' state dict\n",
    "# Why can't this be done in the PPO constructor?\n",
    "for (role, model) in ppo_models.items():\n",
    "    model.init_state_dict(role)\n",
    "\n",
    "# agent = policy/model + method of interaction/learning algorithm\n",
    "# wants to optimize for long-term reward\n",
    "# memory is only required during training of PPO\n",
    "agent = PPO_jax(ppo_models,\n",
    "                memory=RandomMemory_jax(memory_size=1024, num_envs=env.num_envs, device=env.device, replacement=True),  # sampling with replacement\n",
    "                observation_space=env.observation_space, action_space=env.action_space, \n",
    "                cfg=PPO_DEFAULT_CONFIG_JAX.copy())  # configuration dict: hyperparameters, preprocessors, learning rate schedulers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99b4972c-84b0-440c-aa60-e489f2aaf620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:25:17.682098Z",
     "iopub.status.busy": "2023-08-18T10:25:17.681595Z",
     "iopub.status.idle": "2023-08-19T14:18:55.783685Z",
     "shell.execute_reply": "2023-08-19T14:18:55.768386Z",
     "shell.execute_reply.started": "2023-08-18T10:25:17.682098Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                       | 1/100000 [00:00<15:01:04,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': Traced<ShapedArray(int32[1,1])>with<DynamicJaxprTrace(level=1/0)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 15/100000 [00:07<13:54:41,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': Traced<ShapedArray(float32[512,1])>with<DynamicJaxprTrace(level=4/0)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [13:56:44<00:00,  1.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [13:56:53<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training the Agent\n",
    "from skrl.trainers.jax import SequentialTrainer as SequentialTrainer_jax\n",
    "from skrl.trainers.jax import ManualTrainer as ManualTrainer_jax\n",
    "\n",
    "\n",
    "# https://skrl.readthedocs.io/en/latest/intro/getting_started.html#trainers\n",
    "\n",
    "trainer = SequentialTrainer_jax(env=env, agents=[agent])  # default cfg\n",
    "\n",
    "# train the agent\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the agent\n",
    "trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed476d37-420f-4cad-a75a-42a20b8f0442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T20:50:57.415953Z",
     "iopub.status.busy": "2023-08-19T20:50:57.412345Z",
     "iopub.status.idle": "2023-08-19T20:50:57.465504Z",
     "shell.execute_reply": "2023-08-19T20:50:57.465002Z",
     "shell.execute_reply.started": "2023-08-19T20:50:57.414907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MAX_TIMESTEPS = 50_000\n",
    "\n",
    "def play_skrl_episode(env: gym.Env, agent):\n",
    "    (observation, info) = env.reset()  # doesn't make sense for terminated or truncated to be here yet!\n",
    "    #print(observation)\n",
    "    #print(info)\n",
    "    \n",
    "    t = 0  # timestep\n",
    "    (terminated, truncated) = (False, False)  # Episode End = terminated or truncated\n",
    "    while not (terminated or truncated):\n",
    "        (action, value_estimate, act_info) = agent.act(observation, t, DEFAULT_MAX_TIMESTEPS)  # KEY POINT OF RL #\n",
    "        \n",
    "        print(f\"{observation} => {action}: {value_estimate}; ...{act_info}\")\n",
    "        \n",
    "        (observation, reward, terminated, truncated, info) = env.step(action)\n",
    "        \n",
    "        #print(observation)\n",
    "        #print(reward)\n",
    "        #print(terminated)\n",
    "        #print(truncated)\n",
    "        #print(info)\n",
    "        \n",
    "        env.render()\n",
    "        #frame = env.render()\n",
    "        #print(frame)\n",
    "        #plt.imshow(frame)\n",
    "        \n",
    "        t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea035bb2-3ea9-42e0-a8a4-1474c093929b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-19T20:51:08.965448Z",
     "iopub.status.busy": "2023-08-19T20:51:08.965448Z",
     "iopub.status.idle": "2023-08-19T20:51:36.775670Z",
     "shell.execute_reply": "2023-08-19T20:51:36.775670Z",
     "shell.execute_reply.started": "2023-08-19T20:51:08.965448Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[221]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[321]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n",
      "[[421]] => [[0]]: [[nan]]; ...{'net_output': Array([[nan, nan, nan, nan, nan, nan]], dtype=float32), 'stddev': Array([[nan]], dtype=float32)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay_skrl_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 15\u001b[0m, in \u001b[0;36mplay_skrl_episode\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     11\u001b[0m (action, value_estimate, act_info) \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(observation, t, DEFAULT_MAX_TIMESTEPS)  \u001b[38;5;66;03m# KEY POINT OF RL #\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_estimate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; ...\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m (observation, reward, terminated, truncated, info) \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#print(observation)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(reward)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#print(terminated)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(truncated)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#print(info)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\skrl\\envs\\wrappers\\jax\\gymnasium_envs.py:128\u001b[0m, in \u001b[0;36mGymnasiumWrapper.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jax:\n\u001b[0;32m    127\u001b[0m     actions \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_get(actions)\n\u001b[1;32m--> 128\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_to_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# convert response to numpy or jax\u001b[39;00m\n\u001b[0;32m    131\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observation_to_tensor(observation)\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:293\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_mask(s)})\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:323\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\micromambaenv\\envs\\ml_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:449\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    448\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    452\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    453\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "play_skrl_episode(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22a81a6-cc8b-4e16-8e86-fbb83380c215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T10:15:51.559437Z",
     "iopub.status.busy": "2023-08-18T10:15:51.559437Z",
     "iopub.status.idle": "2023-08-18T10:15:51.569584Z",
     "shell.execute_reply": "2023-08-18T10:15:51.569584Z",
     "shell.execute_reply.started": "2023-08-18T10:15:51.559437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommended practice is to save the agents rather than the models\n",
    "runs = [\n",
    "    \"23-08-17_22-42-00-923229_PPO\",\n",
    "    \"23-08-17_23-17-18-255038_PPO\",\n",
    "    \"23-08-18_02-31-14-419606_PPO\"\n",
    "] \n",
    "\n",
    "agent.load(f\"./runs/{runs[0]}/checkpoints/best_agent.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9ddd3-e45a-4c9f-8b02-5dc6a293f213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
